{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/submarinejuice/CP322-Final-Project-Group-9/blob/main/cp322_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Physiological Representation Learning for Predicting Risky Financial Decisions\n",
        "\n",
        "\n",
        "\n",
        "**Research Question** - Can we predict whether a participant will invest in a risky asset on a given trial from:\n",
        "Market context (expected return, volatility)\n",
        "Physiological arousal (aSCRs)\n",
        "\n",
        "**Secondary Research Question:**\n",
        "Do we see comparable physiological signatures of stress/arousal in a real-world wearable dataset (WESAD), and can we learn a shared representation of physiological state that transfers across tasks?\n",
        "\n",
        "**Motivation** - real financial decisions are emotional\n",
        "\n",
        "**Problem** - predicting investment choice from market + physio data\n",
        "\n"
      ],
      "metadata": {
        "id": "jKQJZUx8smAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Setup & Reproducibility"
      ],
      "metadata": {
        "id": "pr5lfb7W8OoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Clone & Pull from our Repository"
      ],
      "metadata": {
        "id": "7LnocH9z83Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")  # optional, makes plots nicer\n",
        "\n",
        "\n",
        "REPO_URL = \"https://github.com/submarinejuice/CP322-Final-Project-Group-9\"\n",
        "REPO_NAME = \"CP322-Final-Project-Group-9\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    # First time in this Colab session: clone the repo\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    # Repo already there in this runtime: pull latest changes\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull\n",
        "    %cd /content\n",
        "\n",
        "# Move into repo so relative paths work\n",
        "%cd /content/{REPO_NAME}\n"
      ],
      "metadata": {
        "id": "a8X3zmLAslA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Kaggle Setup to pull WESAD dataset instead of downloading the data"
      ],
      "metadata": {
        "id": "7IHA4XDR8_mE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Must use your own Kaggle API as documented in the README file. Use your own kaggle.JSON file through the upload prompt that appears when this cell is run."
      ],
      "metadata": {
        "id": "Mkamo0xv9HBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"CWD:\", os.getcwd())\n",
        "os.makedirs('/content/.kaggle', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Upload from laptop\n",
        "uploaded = files.upload()   # select your kaggle.json / kaggle.JSON\n",
        "fname = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", fname)\n",
        "\n",
        "# File is saved in the *current* directory, so src is just fname\n",
        "src = fname\n",
        "dst = '/content/.kaggle/kaggle.json'\n",
        "\n",
        "shutil.move(src, dst)\n",
        "\n",
        "# Fix permissions and inspect\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "id": "DQNE9sXFJfAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.5 Use Google Drive to temporarily store data so you don't have to rerun the download commands for such a large dataset."
      ],
      "metadata": {
        "id": "Cc5w4ltL9VBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y2Ftu6I8f2Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allow notebook to temporarily store WESAD dataset with your drive"
      ],
      "metadata": {
        "id": "X-5OqwWrkqms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading WESAD Dataset once the above has been complete."
      ],
      "metadata": {
        "id": "ftNftWVRkmiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 WESAD download\n",
        " Will skip downloads if you already have the files in your drive."
      ],
      "metadata": {
        "id": "cKflJ4ko9g8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure Kaggle sees the right config\n",
        "import os, shutil\n",
        "\n",
        "print(\"Using config from /content/.kaggle/kaggle.json\")\n",
        "\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/.kaggle\"\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"kaggle.json copied to ~/.kaggle (not printed for security).\")\n"
      ],
      "metadata": {
        "id": "maerzKdsmBJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding an area that will say whether or not you need to redownload the dataset so that you dont double download accidentally !"
      ],
      "metadata": {
        "id": "TjqqqznopIcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to WESAD after unzipping\n",
        "WESAD_DIR = \"data/WESAD\"\n",
        "\n",
        "if os.path.exists(WESAD_DIR) and len(os.listdir(WESAD_DIR)) > 0:\n",
        "    print(\"✔ WESAD dataset already exists. Skipping download.\")\n",
        "else:\n",
        "    print(\"⬇ Downloading WESAD dataset from Kaggle...\")\n",
        "    !kaggle datasets download -d orvile/wesad-wearable-stress-affect-detection-dataset -p data/\n",
        "    !unzip -o \"data/*.zip\" -d data/\n",
        "    print(\"✔ Download complete!\")\n",
        "\n",
        "if not os.path.exists(\"/content/.kaggle/kaggle.json\"):\n",
        "    raise FileNotFoundError(\n",
        "        \"kaggle.json missing — upload via files.upload() before continuing.\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "eNXR70H4pNwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.5 Implementing a loader file"
      ],
      "metadata": {
        "id": "UkbOPHCR-YXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile wesad_loader.py\n",
        "import os\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.errors import EmptyDataError, ParserError\n",
        "\n",
        "WESAD_PATH = \"data/WESAD\"\n",
        "E4_FILES = [\"ACC\", \"BVP\", \"EDA\", \"HR\", \"IBI\", \"TEMP\", \"tags\"]\n",
        "\n",
        "\n",
        "def _read_signal_csv(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Read a 1-column CSV into a 1D numpy array.\n",
        "    If the file is empty or malformed, return an empty array instead of crashing.\n",
        "    \"\"\"\n",
        "    print(f\"[wesad_loader] reading {path}\")\n",
        "    try:\n",
        "        # Quick check by file size\n",
        "        if os.path.getsize(path) == 0:\n",
        "            print(f\"[wesad_loader] WARNING: {path} is 0 bytes (empty)\")\n",
        "            return np.array([])\n",
        "\n",
        "        df = pd.read_csv(path, header=None)\n",
        "        if df.size == 0:\n",
        "            print(f\"[wesad_loader] WARNING: {path} has no values\")\n",
        "            return np.array([])\n",
        "        return df.values.squeeze()\n",
        "\n",
        "    except (EmptyDataError, ParserError) as e:\n",
        "        print(f\"[wesad_loader] WARNING: {path} raised {type(e).__name__}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def load_subject_e4(subject_id: str,\n",
        "                    base_path: str = WESAD_PATH) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load Empatica E4 wrist signals for one subject.\n",
        "    \"\"\"\n",
        "    subj_dir = os.path.join(base_path, subject_id)\n",
        "    e4_dir = os.path.join(subj_dir, f\"{subject_id}_E4_Data\")\n",
        "\n",
        "    if not os.path.isdir(e4_dir):\n",
        "        raise FileNotFoundError(f\"E4 folder not found for {subject_id} at {e4_dir}\")\n",
        "\n",
        "    data: Dict[str, Any] = {}\n",
        "\n",
        "    for name in E4_FILES:\n",
        "        csv_path = os.path.join(e4_dir, f\"{name}.csv\")\n",
        "        if os.path.exists(csv_path):\n",
        "            data[name.lower()] = _read_signal_csv(csv_path)\n",
        "        else:\n",
        "            print(f\"[wesad_loader] WARNING: {csv_path} not found\")\n",
        "            data[name.lower()] = np.array([])\n",
        "\n",
        "    # Parse metadata from info.txt if present\n",
        "    info_path = os.path.join(e4_dir, \"info.txt\")\n",
        "    meta: Dict[str, str] = {}\n",
        "    if os.path.exists(info_path):\n",
        "        with open(info_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or \":\" not in line:\n",
        "                    continue\n",
        "                key, val = [x.strip() for x in line.split(\":\", 1)]\n",
        "                meta[key] = val\n",
        "    data[\"meta\"] = meta\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def list_subjects(base_path: str = WESAD_PATH) -> List[str]:\n",
        "    \"\"\"\n",
        "    List all subject folders like 'S2', 'S3', ...\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(base_path):\n",
        "        raise FileNotFoundError(f\"WESAD base path not found: {base_path}\")\n",
        "    return sorted(\n",
        "        d for d in os.listdir(base_path)\n",
        "        if d.startswith(\"S\") and os.path.isdir(os.path.join(base_path, d))\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ymUrzV-0lSZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Importing loader & list subjects\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YcB4adTOslNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import wesad_loader\n",
        "importlib.reload(wesad_loader)\n",
        "\n",
        "from wesad_loader import load_subject_e4, list_subjects\n",
        "\n",
        "print(os.listdir())  # sanity\n",
        "print(list_subjects(\"data/WESAD\"))\n",
        "\n",
        "s2 = load_subject_e4(\"S2\", base_path=\"data/WESAD\")\n",
        "for k, v in s2.items():\n",
        "    if k == \"meta\":\n",
        "        print(k, v)\n",
        "    else:\n",
        "        print(k, type(v), getattr(v, \"shape\", None))\n"
      ],
      "metadata": {
        "id": "s_3sGZcrsnKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yv3yApQPPk5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was way too large to add to the github, so a way to work around this for reproducibility purposes is to essentially just use your own Kaggle API JSON so u can pull from their database\n",
        "\n",
        "Will go into more detail later, but for now\n",
        "\n",
        "1. Go to Kaggle settings\n",
        "2. Create API token\n",
        "3. Manually bind their own kaggle.JSON if needed\n",
        "4. Upload it the same way to Colab when promted in the cell above.\n"
      ],
      "metadata": {
        "id": "Jy8Ibo3VPeQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2ndary Dataset I used:"
      ],
      "metadata": {
        "id": "uh8SezkK0IGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondary dataset WESAD is 3GB+ and cannot be uploaded to colab or pushed to the git due to such a large file size, so in order to keep this reproducible, I am going to keep this here so that the team will be able to retrieve the data at runtime from the kaggle servers."
      ],
      "metadata": {
        "id": "jp9ou8zGDGAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wesad_eda_example(subject_data, n_seconds=300, fs=4):\n",
        "    \"\"\"\n",
        "    Plot a snippet of EDA signal for one WESAD subject.\n",
        "    - subject_data: dict returned by load_subject_e4(...)\n",
        "    - n_seconds: length of snippet to show\n",
        "    - fs: sampling frequency for EDA (Hz). WESAD EDA is usually 4 Hz.\n",
        "    \"\"\"\n",
        "    eda = subject_data.get(\"eda\", None)\n",
        "    if eda is None or eda.size == 0:\n",
        "        print(\"No EDA data found.\")\n",
        "        return\n",
        "\n",
        "    n_samples = min(len(eda), n_seconds * fs)\n",
        "    eda_snippet = eda[:n_samples]\n",
        "\n",
        "    # Build time axis in seconds\n",
        "    t = np.arange(n_samples) / fs\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(t, eda_snippet)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"EDA (µS)\")\n",
        "    plt.title(f\"WESAD: EDA snippet (first {n_seconds} seconds)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_wesad_eda_example(s2, n_seconds=300, fs=4)\n"
      ],
      "metadata": {
        "id": "ogW0iQwTX8gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand physiological arousal, we first visualized raw EDA from WESAD. This plot shows 300 seconds of continuous skin-conductance from one subject. The fluctuations show moment-to-moment arousal changes something the AE dataset is missing. This justified using WESAD as a complementary source to learn general arousal patterns."
      ],
      "metadata": {
        "id": "u5KbEin8YtrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Affective Economics Dataset\n",
        "---\n"
      ],
      "metadata": {
        "id": "W-4ErWzj_EYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Repo contents:\", os.listdir())\n",
        "print(\"DATASET contents:\", os.listdir(\"DATASET\"))\n",
        "\n",
        "df = pd.read_csv(\"DATASET/AE_investment_dataset.csv\")\n",
        "df.head()\n",
        "df.info()\n",
        "df.isna().mean().sort_values().head(20)\n",
        "df.columns.tolist()\n",
        "for c in df.columns:\n",
        "    print(c)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHgjKOa5tQe9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quick note bc I didn't know what PANAS meant:\n",
        "- PANAS refers to the Positive and Negative Affect Schedule, a widely used psychological scale that measures an individual's mood by assessing both positive and negative emotions. Developed in 1988 by Watson, Clark, and Tellegen, it's a 20-item self-report measure used in research and clinical settings to gauge how frequently someone experiences emotions like interest, joy, enthusiasm (positive affect) versus feelings of distress, sadness, and nervousness (negative affect).\n",
        "## How it works\n",
        "- 20 items: The scale consists of 20 words that describe different feelings and emotions.\n",
        "- Two dimensions: These items are separated into two subscales: one for positive affect (PA) and one for negative affect (NA).\n",
        "- Rating scale: Participants rate how they felt about each item over a specific time frame (e.g., \"right now,\" \"today,\" \"over the past few weeks\") on a 5-point scale.\n",
        "- Scoring: Each positive and negative item is scored individually. The total positive score and total negative score are then calculated. A higher positive score indicates more positive affect, while a higher negative score indicates more negative affect."
      ],
      "metadata": {
        "id": "KPuQaXQjy8wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a per-trial table with:\n",
        "1. inputs per step:\n",
        "  - money_in_stocks\n",
        "  - mean_return\n",
        "  - stock_fluctuation\n",
        "  - scr_anticipatory\n",
        "2. Static inputs:\n",
        "3. Target\n",
        "  - Whether they invested in the stock (money_in_stocks > 0 -> 1 else 0)"
      ],
      "metadata": {
        "id": "xJg0a9hQw5Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle Dataset for WESAD\n",
        "\n",
        "-- note: ref doc later"
      ],
      "metadata": {
        "id": "pU7JKcLUQ-Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal WESAD pipeline\n",
        "- loading dataset, segmenting data into windows, computing basics, labelling windows as baselines vs stress.\n",
        "- using dataframe and constructing just like we do later on for the Bath dataset"
      ],
      "metadata": {
        "id": "kGH50BNvm3q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Loading Affective Economics Dataset\n",
        "---"
      ],
      "metadata": {
        "id": "NCFO4zqp_aRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Repo contents:\", os.listdir())\n",
        "print(\"DATASET contents:\", os.listdir(\"DATASET\"))\n",
        "\n",
        "df = pd.read_csv(\"DATASET/AE_investment_dataset.csv\")\n",
        "df.info()\n",
        "\n",
        "# 0. ID & static columns\n",
        "id_cols = [\"Participant_code\", \"Age\", \"Gender\", \"Nationality\", \"Ethnicity\", \"Played_stock_market\"]\n",
        "\n",
        "# 1. Grab trial-level columns by prefix\n",
        "stock_cols = [c for c in df.columns if c.startswith(\"Money_in_stocks_S\")]\n",
        "scr_cols   = [c for c in df.columns if c.startswith(\"SCR_AnticipatoryS\")]\n",
        "ret_cols   = [c for c in df.columns if c.startswith(\"Mean_Return_S\")]\n",
        "fluc_cols  = [c for c in df.columns if c.startswith(\"stock_fluctuation_S\")]\n",
        "\n",
        "print(\"n_stock_cols:\", len(stock_cols))\n",
        "print(\"n_scr_cols:\", len(scr_cols))\n",
        "print(\"n_return_cols:\", len(ret_cols))\n",
        "print(\"n_fluctuation_cols:\", len(fluc_cols))\n",
        "\n",
        "# 2. Map (session, trial) -> column name\n",
        "def build_lookup(cols, prefix):\n",
        "    lookup = {}\n",
        "    for c in cols:\n",
        "        # e.g. Money_in_stocks_S1_T3  →  session=1, trial=3\n",
        "        m = re.match(rf\"{re.escape(prefix)}(\\d+)_T(\\d+)$\", c)\n",
        "        if m:\n",
        "            s = int(m.group(1))   # session number\n",
        "            t = int(m.group(2))   # trial number within session\n",
        "            lookup[(s, t)] = c\n",
        "    return lookup\n",
        "\n",
        "stock_map = build_lookup(stock_cols, \"Money_in_stocks_S\")\n",
        "scr_map   = build_lookup(scr_cols,   \"SCR_AnticipatoryS\")\n",
        "ret_map   = build_lookup(ret_cols,   \"Mean_Return_S\")\n",
        "fluc_map  = build_lookup(fluc_cols,  \"stock_fluctuation_S\")\n",
        "\n",
        "print(\"number of keys in stock_map:\", len(stock_map))\n",
        "print(\"some keys from stock_map:\", list(stock_map.items())[:5])\n"
      ],
      "metadata": {
        "id": "QSaZ40fmqL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Long format trial table"
      ],
      "metadata": {
        "id": "84nI2l_vqNg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aD-mtCv7_vS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # carry participant-level info\n",
        "    base = {col: row[col] for col in id_cols}\n",
        "\n",
        "    for (s, t) in sorted(stock_map.keys()):\n",
        "        rec = dict(base)\n",
        "        rec[\"session\"] = s\n",
        "        rec[\"trial_in_session\"] = t\n",
        "        rec[\"global_trial\"] = (s - 1) * 10 + t  # 1..40\n",
        "\n",
        "        rec[\"money_in_stocks\"] = row[stock_map[(s, t)]]\n",
        "        rec[\"scr_anticipatory\"] = row[scr_map[(s, t)]]\n",
        "\n",
        "        # Some (session, trial) combos might not have return/fluctuation\n",
        "        rec[\"mean_return\"] = row[ret_map[(s, t)]] if (s, t) in ret_map else pd.NA\n",
        "        rec[\"stock_fluctuation\"] = row[fluc_map[(s, t)]] if (s, t) in fluc_map else pd.NA\n",
        "\n",
        "        rows.append(rec)\n",
        "\n",
        "long_df = pd.DataFrame(rows)\n",
        "\n",
        "# Target: did they invest at all?\n",
        "long_df[\"invested\"] = (long_df[\"money_in_stocks\"] > 0).astype(int)\n",
        "\n",
        "print(long_df.shape)\n",
        "long_df.head()\n",
        "print(long_df[\"invested\"].value_counts())\n"
      ],
      "metadata": {
        "id": "FhdtP55eqN1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Cleaning + Basic Features\n"
      ],
      "metadata": {
        "id": "qebdov_vqPgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Drop rows where our key features are missing\n",
        "key_features = [\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\", \"money_in_stocks\"]\n",
        "\n",
        "clean_df = long_df.dropna(subset=key_features).copy()\n",
        "\n",
        "# Convert types to numeric\n",
        "for col in key_features:\n",
        "    clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
        "\n",
        "# Drop again if any become NA\n",
        "clean_df = clean_df.dropna(subset=key_features)\n",
        "\n",
        "# Target\n",
        "y = clean_df[\"invested\"].astype(int)\n",
        "\n",
        "# Features: minimal baseline\n",
        "X = clean_df[[\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\"]]\n",
        "\n",
        "print(\"Clean shape:\", clean_df.shape)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "tOjbGQh-qPwK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "725b229f-9a21-4455-8e99-83d9661a7067"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'long_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3582910664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkey_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"scr_anticipatory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean_return\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stock_fluctuation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"money_in_stocks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclean_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Convert types to numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'long_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.4 Ablation"
      ],
      "metadata": {
        "id": "x_XVLJQXSEL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "def train_eval_logreg(X, y, desc):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1  = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n=== {desc} ===\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "y = clean_df[\"invested\"].astype(int)\n",
        "\n",
        "# Physiology-only\n",
        "X_phys = clean_df[[\"scr_anticipatory\"]]\n",
        "\n",
        "# Market-only\n",
        "X_market = clean_df[[\"mean_return\", \"stock_fluctuation\"]]\n",
        "\n",
        "# Combined\n",
        "X_both = clean_df[[\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\"]]\n",
        "\n",
        "acc_phys, f1_phys = train_eval_logreg(X_phys, y, \"Physiology only (SCR)\")\n",
        "acc_mark, f1_mark = train_eval_logreg(X_market, y, \"Market context only\")\n",
        "acc_both, f1_both = train_eval_logreg(X_both, y, \"Physiology + Market\")\n"
      ],
      "metadata": {
        "id": "8TulFOyTS7Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Correlation Heatmap of Models\n",
        " What it shows:\n",
        "SCR positively correlated with investing\n",
        "Returns/volatility relationships\n",
        "Good for “Feature Analysis” slide"
      ],
      "metadata": {
        "id": "1gKZWSVbZg3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr = clean_df[['scr_anticipatory','mean_return','stock_fluctuation','invested']].corr()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"AE Dataset — Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ELk3SipxZjDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Baseline model\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "L4zrnPS4wpo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bqyfyroHwr5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Scale features again (safe to reuse)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "k_bUH3aZwsOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Cell\n"
      ],
      "metadata": {
        "id": "_nzl-aYc37DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#scale data again\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "JZDBGFsQ3843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Work on a copy, sorted by participant + time\n",
        "scr_df = clean_df.sort_values([\"Participant_code\", \"session\", \"trial_in_session\"]).copy()\n",
        "\n",
        "grp = scr_df.groupby(\"Participant_code\")\n",
        "\n",
        "# Participant-level mean/std and z-score\n",
        "scr_df[\"scr_mean_p\"] = grp[\"scr_anticipatory\"].transform(\"mean\")\n",
        "scr_df[\"scr_std_p\"]  = grp[\"scr_anticipatory\"].transform(\"std\")\n",
        "scr_df[\"scr_z\"] = (scr_df[\"scr_anticipatory\"] - scr_df[\"scr_mean_p\"]) / scr_df[\"scr_std_p\"]\n",
        "\n",
        "# Lags within each participant\n",
        "scr_df[\"scr_lag1\"] = grp[\"scr_anticipatory\"].shift(1)\n",
        "scr_df[\"scr_lag2\"] = grp[\"scr_anticipatory\"].shift(2)\n",
        "\n",
        "# Changes vs previous trials\n",
        "scr_df[\"scr_delta1\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag1\"]\n",
        "scr_df[\"scr_delta2\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag2\"]\n",
        "\n",
        "# Short-term rolling window stats (window=3 trials)\n",
        "scr_df[\"scr_roll_mean3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        ")\n",
        "scr_df[\"scr_roll_std3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "\n",
        "# Replace NaNs from lags / std=0 with 0 for now\n",
        "scr_feature_cols = [\n",
        "    \"scr_anticipatory\",\n",
        "    \"scr_z\",\n",
        "    \"scr_lag1\", \"scr_lag2\",\n",
        "    \"scr_delta1\", \"scr_delta2\",\n",
        "    \"scr_roll_mean3\", \"scr_roll_std3\",\n",
        "]\n",
        "\n",
        "scr_df[scr_feature_cols] = scr_df[scr_feature_cols].fillna(0.0)\n",
        "\n",
        "print(\"SCR feature shape:\", scr_df[scr_feature_cols].shape)\n",
        "scr_df[scr_feature_cols + [\"invested\"]].head()\n"
      ],
      "metadata": {
        "id": "0RYaY--sCQKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Plotting AE: SCR + investing over trials for one participant\n"
      ],
      "metadata": {
        "id": "dJFSLZtACP8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ae_scr_timeseries(scr_df, participant_code):\n",
        "    \"\"\"\n",
        "    Plot anticipatory SCR across trials for one participant,\n",
        "    with invest vs not-invest marked.\n",
        "    \"\"\"\n",
        "    # Filter and sort in time order\n",
        "    subj = (\n",
        "        scr_df[scr_df[\"Participant_code\"] == participant_code]\n",
        "        .sort_values([\"session\", \"trial_in_session\"])\n",
        "        .copy()\n",
        "    )\n",
        "    if subj.empty:\n",
        "        print(f\"No data for Participant_code={participant_code}\")\n",
        "        return\n",
        "\n",
        "    # Build a global index (1..N) just for plotting\n",
        "    subj[\"trial_idx\"] = range(1, len(subj) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    # Line of SCR\n",
        "    plt.plot(subj[\"trial_idx\"], subj[\"scr_anticipatory\"], marker=\"o\", label=\"SCR\")\n",
        "\n",
        "    # Mark invest vs not-invest\n",
        "    invested = subj[\"invested\"] == 1\n",
        "    not_invested = subj[\"invested\"] == 0\n",
        "\n",
        "    plt.scatter(\n",
        "        subj.loc[invested, \"trial_idx\"],\n",
        "        subj.loc[invested, \"scr_anticipatory\"],\n",
        "        marker=\"^\",\n",
        "        label=\"Invested (1)\",\n",
        "    )\n",
        "    plt.scatter(\n",
        "        subj.loc[not_invested, \"trial_idx\"],\n",
        "        subj.loc[not_invested, \"scr_anticipatory\"],\n",
        "        marker=\"x\",\n",
        "        label=\"Not invested (0)\",\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Trial (time)\")\n",
        "    plt.ylabel(\"Anticipatory SCR\")\n",
        "    plt.title(f\"AE: SCR and investment decisions over trials (Participant {participant_code})\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "G-SIKgkPXRNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick one example participant from the data\n",
        "example_pid = scr_df[\"Participant_code\"].iloc[0]\n",
        "plot_ae_scr_timeseries(scr_df, example_pid)\n"
      ],
      "metadata": {
        "id": "tHJHQ1m2XVI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This figure shows anticipatory SCR (emotion/arousal) for a single AE participant plotted across the 40 investment trials. We annotated trials where the participant invested versus not. We observed that spikes in SCR often precede investment events, consistent with findings from affective neuroscience: anticipatory arousal tracks risky decision engagement."
      ],
      "metadata": {
        "id": "BIAt0sfOY4yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Plotting AE with Market Context (Mean Return & Volatility) over trials"
      ],
      "metadata": {
        "id": "bHr1kwIRXZpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ae_market_timeseries(scr_df, participant_code):\n",
        "    \"\"\"\n",
        "    Plot mean return and stock fluctuation over trials for one participant.\n",
        "    \"\"\"\n",
        "    subj = (\n",
        "        scr_df[scr_df[\"Participant_code\"] == participant_code]\n",
        "        .sort_values([\"session\", \"trial_in_session\"])\n",
        "        .copy()\n",
        "    )\n",
        "    if subj.empty:\n",
        "        print(f\"No data for Participant_code={participant_code}\")\n",
        "        return\n",
        "\n",
        "    subj[\"trial_idx\"] = range(1, len(subj) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(\n",
        "        subj[\"trial_idx\"], subj[\"mean_return\"],\n",
        "        marker=\"o\", label=\"Mean return\"\n",
        "    )\n",
        "    plt.plot(\n",
        "        subj[\"trial_idx\"], subj[\"stock_fluctuation\"],\n",
        "        marker=\"s\", label=\"Stock fluctuation\"\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Trial (time)\")\n",
        "    plt.ylabel(\"Market values\")\n",
        "    plt.title(f\"AE: Market context over trials (Participant {participant_code})\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_ae_market_timeseries(scr_df, example_pid)\n"
      ],
      "metadata": {
        "id": "lvU3OPTlXilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ==== Prepare data ====\n",
        "X_scr = scr_df[scr_feature_cols].values.astype(\"float32\")\n",
        "y_scr = scr_df[\"invested\"].values.astype(\"int64\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scr, y_scr, test_size=0.2, random_state=42, stratify=y_scr\n",
        ")\n",
        "\n",
        "# Standardize SCR features\n",
        "scaler_scr = StandardScaler()\n",
        "X_train_scaled = scaler_scr.fit_transform(X_train)\n",
        "X_val_scaled   = scaler_scr.transform(X_val)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor   = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_ds   = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "emb_dim   = 16\n",
        "\n",
        "# ==== Physiology encoder ====\n",
        "class PhysioEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=32, emb_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Full model: encoder + classifier head\n",
        "class PhysioModel(nn.Module):\n",
        "    def __init__(self, encoder, emb_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Linear(emb_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)        # [batch, emb_dim]\n",
        "        logits = self.classifier(z)  # [batch, 2]\n",
        "        return logits\n",
        "\n",
        "encoder = PhysioEncoder(input_dim, hidden_dim=32, emb_dim=emb_dim)\n",
        "model = PhysioModel(encoder, emb_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ==== Training loop ====\n",
        "n_epochs = 30\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_true  = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_true.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_true  = torch.cat(all_true).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_true, all_preds)\n",
        "    f1  = f1_score(all_true, all_preds)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "              f\"train_loss={np.mean(train_losses):.4f} | \"\n",
        "              f\"val_acc={acc:.3f} | val_f1={f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "dwpP5lQ3CTEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.1 Evaluate"
      ],
      "metadata": {
        "id": "I13k4njKTLVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation helper\n"
      ],
      "metadata": {
        "id": "VVa68r7GTNhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_physio_model(model, X_scaled, y_true):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(X_scaled, dtype=torch.float32).to(device))\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(y_true, preds)\n",
        "    f1  = f1_score(y_true, preds)\n",
        "    print(\"PhysioEncoder Accuracy:\", acc)\n",
        "    print(\"PhysioEncoder F1:\", f1)\n",
        "    return acc, f1\n",
        "\n",
        "physio_acc, physio_f1 = evaluate_physio_model(model, X_val_scaled, y_val)\n"
      ],
      "metadata": {
        "id": "IK6ZvmMqTPL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Feature Pipeline and Classifer"
      ],
      "metadata": {
        "id": "YLDLPn-2Tpzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def make_eda_windows(eda, window_sec=60, step_sec=30, fs=4):\n",
        "    \"\"\"\n",
        "    Convert 1D EDA signal into overlapping windows.\n",
        "    - window_sec: window length in seconds\n",
        "    - step_sec: step size between windows in seconds\n",
        "    - fs: sampling rate (Hz) for EDA (WESAD E4 EDA = 4 Hz)\n",
        "    \"\"\"\n",
        "    # EDA in WESAD E4 has first two rows = [start_time, sampling_rate]\n",
        "    # Strip them out:\n",
        "    if len(eda) > 2:\n",
        "        eda_values = eda[2:].astype(float)\n",
        "    else:\n",
        "        return np.empty((0, 5)), np.empty((0,), dtype=int)\n",
        "\n",
        "    win_len = window_sec * fs\n",
        "    step_len = step_sec * fs\n",
        "\n",
        "    windows = []\n",
        "    for start in range(0, len(eda_values) - win_len, step_len):\n",
        "        seg = eda_values[start:start + win_len]\n",
        "        if len(seg) < win_len:\n",
        "            break\n",
        "        # basic stats\n",
        "        mean = seg.mean()\n",
        "        std  = seg.std()\n",
        "        max_ = seg.max()\n",
        "        min_ = seg.min()\n",
        "        slope = (seg[-1] - seg[0]) / window_sec  # change per second\n",
        "        windows.append([mean, std, max_, min_, slope])\n",
        "\n",
        "    if not windows:\n",
        "        return np.empty((0, 5)), np.empty((0,), dtype=int)\n",
        "\n",
        "    windows = np.array(windows)\n",
        "\n",
        "    # Proxy high/low arousal via z-score of mean EDA across windows\n",
        "    z = (windows[:, 0] - windows[:, 0].mean()) / (windows[:, 0].std() + 1e-8)\n",
        "    labels = (z > 0.5).astype(int)  # 1 = “higher arousal”, 0 = “lower arousal”\n",
        "\n",
        "    return windows, labels\n"
      ],
      "metadata": {
        "id": "4e1YDLq1Tlud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Dataset Across multiple subjects"
      ],
      "metadata": {
        "id": "y6FpHModTuUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "subjects = list_subjects(\"data/WESAD\")\n",
        "print(\"Subjects:\", subjects)\n",
        "\n",
        "for sid in subjects:\n",
        "    s = load_subject_e4(sid, base_path=\"data/WESAD\")\n",
        "    eda = s[\"eda\"]\n",
        "    X_win, y_win = make_eda_windows(eda)\n",
        "    if len(X_win) == 0:\n",
        "        continue\n",
        "    all_X.append(X_win)\n",
        "    all_y.append(y_win)\n",
        "\n",
        "X_eda = np.vstack(all_X)\n",
        "y_eda = np.concatenate(all_y)\n",
        "\n",
        "print(\"WESAD windowed EDA shape:\", X_eda.shape, \"labels:\", np.bincount(y_eda))\n"
      ],
      "metadata": {
        "id": "B-l4Cv22TxVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Classifier"
      ],
      "metadata": {
        "id": "E2s_VyxmT0V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
        "    X_eda, y_eda, test_size=0.2, random_state=42, stratify=y_eda\n",
        ")\n",
        "\n",
        "scaler_w = StandardScaler()\n",
        "X_train_w_scaled = scaler_w.fit_transform(X_train_w)\n",
        "X_test_w_scaled  = scaler_w.transform(X_test_w)\n",
        "\n",
        "clf_w = LogisticRegression(max_iter=1000)\n",
        "clf_w.fit(X_train_w_scaled, y_train_w)\n",
        "y_pred_w = clf_w.predict(X_test_w_scaled)\n",
        "\n",
        "print(\"WESAD EDA baseline Accuracy:\", accuracy_score(y_test_w, y_pred_w))\n",
        "print(\"WESAD EDA baseline F1:\", f1_score(y_test_w, y_pred_w))\n",
        "print(classification_report(y_test_w, y_pred_w))\n"
      ],
      "metadata": {
        "id": "-glXqK0RT1_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AE: we model trial-level anticipatory SCR + market context → risky vs safe choice\n",
        "WESAD: we model time-windowed continuous EDA → low vs high arousal (proxy labels)\n",
        "Both pipelines use small neural models on physiological features.\n",
        "This justifies the phrase “multimodal physiological representation learning”.\n",
        "Is the WESAD label scheme “perfect”? No. But for a course project, if you’re explicit (“weak supervision via EDA z-score as arousal proxy”), it’s absolutely fine."
      ],
      "metadata": {
        "id": "y4FqnitCT5eN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results & Write-up"
      ],
      "metadata": {
        "id": "bvfyj7ERXrI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_investment_class_distribution(clean_df):\n",
        "    counts = clean_df[\"invested\"].value_counts().sort_index()\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    counts.plot(kind=\"bar\")\n",
        "    plt.xticks([0, 1], [\"Not invested (0)\", \"Invested (1)\"], rotation=0)\n",
        "    plt.ylabel(\"Number of trials\")\n",
        "    plt.title(\"Class distribution of investment decisions\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(counts)\n",
        "\n",
        "plot_investment_class_distribution(clean_df)\n"
      ],
      "metadata": {
        "id": "Y8nTQGnLXt9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### Confusion Matrix"
      ],
      "metadata": {
        "id": "hHP1z_pkZqF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_mlp)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Invest\", \"Invest\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"MLP Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i4VBrG2OZtmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ablation Plots (Market-only vs SCR-only vs both)"
      ],
      "metadata": {
        "id": "YocOsNenZyQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MARKET-ONLY FEATURES\n",
        "X_mkt = clean_df[['mean_return','stock_fluctuation']]\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_mkt, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler_m = StandardScaler()\n",
        "X_train_m = scaler_m.fit_transform(X_train_m)\n",
        "X_test_m  = scaler_m.transform(X_test_m)\n",
        "\n",
        "log_m = LogisticRegression().fit(X_train_m, y_train_m)\n",
        "acc_market = accuracy_score(y_test_m, log_m.predict(X_test_m))\n",
        "\n",
        "# SCR-ONLY FEATURES\n",
        "X_scr_only = clean_df[['scr_anticipatory']]\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_scr_only, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler_s = StandardScaler()\n",
        "X_train_s = scaler_s.fit_transform(X_train_s)\n",
        "X_test_s  = scaler_s.transform(X_test_s)\n",
        "\n",
        "log_s = LogisticRegression().fit(X_train_s, y_train_s)\n",
        "acc_scr = accuracy_score(y_test_s, log_s.predict(X_test_s))\n",
        "\n",
        "# COMBINED (already done)\n",
        "acc_combined = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#now plot it\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([\"Market only\", \"SCR only\", \"Combined\"], [acc_market, acc_scr, acc_combined],\n",
        "        color=[\"#6baed6\", \"#74c476\", \"#fd8d3c\"])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Ablation Study — Contribution of Each Feature Set\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oPTJLhr7Z2OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Loss Curve"
      ],
      "metadata": {
        "id": "qrPeymVIZ71L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_accs = []\n",
        "val_f1s = []\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    batch_losses = []\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_losses.append(loss.item())\n",
        "    train_losses.append(np.mean(batch_losses))\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    all_preds, all_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_true.append(yb.cpu())\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_true  = torch.cat(all_true).numpy()\n",
        "    val_accs.append(accuracy_score(all_true, all_preds))\n",
        "    val_f1s.append(f1_score(all_true, all_preds))\n",
        "\n",
        "    #plot it\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.title(\"MLP Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(val_accs, label=\"Val Accuracy\")\n",
        "plt.plot(val_f1s, label=\"Val F1\")\n",
        "plt.title(\"MLP Validation Curves\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0F5-JyMjZ-qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further analysis, because we have only been plotting for one prticipant, here is SCR distribution across ALL participants."
      ],
      "metadata": {
        "id": "xTsy1-aqaGiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(clean_df[\"scr_anticipatory\"], bins=30, color=\"#9ecae1\", edgecolor=\"black\")\n",
        "plt.title(\"Distribution of SCR Across All Participants\")\n",
        "plt.xlabel(\"SCR (µS)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vROeZJANaNoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is Average SCR by participant"
      ],
      "metadata": {
        "id": "qKvfrSClaQcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_scr = scr_df.groupby(\"Participant_code\")[\"scr_anticipatory\"].mean()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "avg_scr.plot(kind=\"bar\", color=\"#74c476\")\n",
        "plt.title(\"Average SCR per Participant\")\n",
        "plt.xlabel(\"Participant ID\")\n",
        "plt.ylabel(\"Mean SCR\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tr-fy01RaTgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investment Rate by Participant"
      ],
      "metadata": {
        "id": "wZRROj7iaX4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inv = scr_df.groupby(\"Participant_code\")[\"invested\"].mean()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "inv.plot(kind=\"bar\", color=\"#fd8d3c\")\n",
        "plt.title(\"Investment Rate per Participant\")\n",
        "plt.xlabel(\"Participant ID\")\n",
        "plt.ylabel(\"% Invested\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VK8UJ_bYaaSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relationship Plot For SCR -> Probability of investment"
      ],
      "metadata": {
        "id": "EyYeEGk1acbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.regplot(x=scr_df[\"scr_anticipatory\"], y=scr_df[\"invested\"],\n",
        "            logistic=True, scatter_kws={\"alpha\":0.3})\n",
        "plt.title(\"Logistic Relationship Between SCR and Investment Probability\")\n",
        "plt.xlabel(\"SCR (µS)\")\n",
        "plt.ylabel(\"P(Invest)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0chEO_soahZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}