{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/submarinejuice/CP322-Final-Project-Group-9/blob/Michelle-Main/cp322_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Physiological Representation Learning for Predicting Risky Financial Decisions\n",
        "\n",
        "\n",
        "\n",
        "**Research Question** - Can we predict whether a participant will invest in a risky asset on a given trial from:\n",
        "Market context (expected return, volatility)\n",
        "Physiological arousal (aSCRs)\n",
        "\n",
        "**Secondary Research Question:**\n",
        "Do we see comparable physiological signatures of stress/arousal in a real-world wearable dataset (WESAD), and can we learn a shared representation of physiological state that transfers across tasks?\n",
        "\n",
        "**Motivation** - real financial decisions are emotional\n",
        "\n",
        "**Problem** - predicting investment choice from market + physio data\n",
        "\n"
      ],
      "metadata": {
        "id": "jKQJZUx8smAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Setup & Reproducibility"
      ],
      "metadata": {
        "id": "pr5lfb7W8OoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Clone & Pull from our Repository"
      ],
      "metadata": {
        "id": "7LnocH9z83Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/submarinejuice/CP322-Final-Project-Group-9\"\n",
        "REPO_NAME = \"CP322-Final-Project-Group-9\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    # First time in this Colab session: clone the repo\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    # Repo already there in this runtime: pull latest changes\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull\n",
        "    %cd /content\n",
        "\n",
        "# Move into repo so relative paths work\n",
        "%cd /content/{REPO_NAME}\n"
      ],
      "metadata": {
        "id": "a8X3zmLAslA5",
        "outputId": "fd8e0e97-3955-4597-c386-f3e63cbb8ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CP322-Final-Project-Group-9/CP322-Final-Project-Group-9\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 1.46 KiB | 750.00 KiB/s, done.\n",
            "From https://github.com/submarinejuice/CP322-Final-Project-Group-9\n",
            "   36c779b..5c7cbd3  Michelle-Main -> origin/Michelle-Main\n",
            "Already up to date.\n",
            "/content\n",
            "/content/CP322-Final-Project-Group-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Kaggle Setup to pull WESAD dataset instead of downloading the data"
      ],
      "metadata": {
        "id": "7IHA4XDR8_mE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Must use your own Kaggle API as documented in the README file. Use your own kaggle.JSON file through the upload prompt that appears when this cell is run."
      ],
      "metadata": {
        "id": "Mkamo0xv9HBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"CWD:\", os.getcwd())\n",
        "os.makedirs('/content/.kaggle', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Upload from laptop\n",
        "uploaded = files.upload()   # select your kaggle.json / kaggle.JSON\n",
        "fname = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", fname)\n",
        "\n",
        "# File is saved in the *current* directory, so src is just fname\n",
        "src = fname\n",
        "dst = '/content/.kaggle/kaggle.json'\n",
        "\n",
        "shutil.move(src, dst)\n",
        "\n",
        "# Fix permissions and inspect\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "DQNE9sXFJfAu",
        "outputId": "1c87b65a-3ef0-4ac8-804a-3819d30ccc70"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "CWD: /content/CP322-Final-Project-Group-9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a71b5005-1edc-4399-811d-8a82f2a79f12\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a71b5005-1edc-4399-811d-8a82f2a79f12\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.JSON to kaggle.JSON\n",
            "Uploaded: kaggle.JSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.5 Use Google Drive to temporarily store data so you don't have to rerun the download commands for such a large dataset."
      ],
      "metadata": {
        "id": "Cc5w4ltL9VBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y2Ftu6I8f2Rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d3de84-b454-46cf-f6e8-0e4de765236e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allow notebook to temporarily store WESAD dataset with your drive"
      ],
      "metadata": {
        "id": "X-5OqwWrkqms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading WESAD Dataset once the above has been complete."
      ],
      "metadata": {
        "id": "ftNftWVRkmiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 WESAD download\n",
        " Will skip downloads if you already have the files in your drive."
      ],
      "metadata": {
        "id": "cKflJ4ko9g8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure Kaggle sees the right config\n",
        "import os, shutil\n",
        "\n",
        "print(\"Using config from /content/.kaggle/kaggle.json\")\n",
        "\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/.kaggle\"\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"kaggle.json copied to ~/.kaggle (not printed for security).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maerzKdsmBJ3",
        "outputId": "12f940e2-1608-4b22-a9ca-084ec68722f4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using config from /content/.kaggle/kaggle.json\n",
            "kaggle.json copied to ~/.kaggle (not printed for security).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding an area that will say whether or not you need to redownload the dataset so that you dont double download accidentally !"
      ],
      "metadata": {
        "id": "TjqqqznopIcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to WESAD after unzipping\n",
        "WESAD_DIR = \"data/WESAD\"\n",
        "\n",
        "if os.path.exists(WESAD_DIR) and len(os.listdir(WESAD_DIR)) > 0:\n",
        "    print(\"✔ WESAD dataset already exists. Skipping download.\")\n",
        "else:\n",
        "    print(\"⬇ Downloading WESAD dataset from Kaggle...\")\n",
        "    !kaggle datasets download -d orvile/wesad-wearable-stress-affect-detection-dataset -p data/\n",
        "    !unzip -o \"data/*.zip\" -d data/\n",
        "    print(\"✔ Download complete!\")\n",
        "\n",
        "if not os.path.exists(\"/content/.kaggle/kaggle.json\"):\n",
        "    raise FileNotFoundError(\n",
        "        \"kaggle.json missing — upload via files.upload() before continuing.\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNXR70H4pNwm",
        "outputId": "8a7f77a8-7410-4cdb-f421-c4c9afa61aa7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ WESAD dataset already exists. Skipping download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.5 Implementing a loader file"
      ],
      "metadata": {
        "id": "UkbOPHCR-YXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile wesad_loader.py\n",
        "import os\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.errors import EmptyDataError, ParserError\n",
        "\n",
        "WESAD_PATH = \"data/WESAD\"\n",
        "E4_FILES = [\"ACC\", \"BVP\", \"EDA\", \"HR\", \"IBI\", \"TEMP\", \"tags\"]\n",
        "\n",
        "\n",
        "def _read_signal_csv(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Read a 1-column CSV into a 1D numpy array.\n",
        "    If the file is empty or malformed, return an empty array instead of crashing.\n",
        "    \"\"\"\n",
        "    print(f\"[wesad_loader] reading {path}\")\n",
        "    try:\n",
        "        # Quick check by file size\n",
        "        if os.path.getsize(path) == 0:\n",
        "            print(f\"[wesad_loader] WARNING: {path} is 0 bytes (empty)\")\n",
        "            return np.array([])\n",
        "\n",
        "        df = pd.read_csv(path, header=None)\n",
        "        if df.size == 0:\n",
        "            print(f\"[wesad_loader] WARNING: {path} has no values\")\n",
        "            return np.array([])\n",
        "        return df.values.squeeze()\n",
        "\n",
        "    except (EmptyDataError, ParserError) as e:\n",
        "        print(f\"[wesad_loader] WARNING: {path} raised {type(e).__name__}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def load_subject_e4(subject_id: str,\n",
        "                    base_path: str = WESAD_PATH) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load Empatica E4 wrist signals for one subject.\n",
        "    \"\"\"\n",
        "    subj_dir = os.path.join(base_path, subject_id)\n",
        "    e4_dir = os.path.join(subj_dir, f\"{subject_id}_E4_Data\")\n",
        "\n",
        "    if not os.path.isdir(e4_dir):\n",
        "        raise FileNotFoundError(f\"E4 folder not found for {subject_id} at {e4_dir}\")\n",
        "\n",
        "    data: Dict[str, Any] = {}\n",
        "\n",
        "    for name in E4_FILES:\n",
        "        csv_path = os.path.join(e4_dir, f\"{name}.csv\")\n",
        "        if os.path.exists(csv_path):\n",
        "            data[name.lower()] = _read_signal_csv(csv_path)\n",
        "        else:\n",
        "            print(f\"[wesad_loader] WARNING: {csv_path} not found\")\n",
        "            data[name.lower()] = np.array([])\n",
        "\n",
        "    # Parse metadata from info.txt if present\n",
        "    info_path = os.path.join(e4_dir, \"info.txt\")\n",
        "    meta: Dict[str, str] = {}\n",
        "    if os.path.exists(info_path):\n",
        "        with open(info_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or \":\" not in line:\n",
        "                    continue\n",
        "                key, val = [x.strip() for x in line.split(\":\", 1)]\n",
        "                meta[key] = val\n",
        "    data[\"meta\"] = meta\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def list_subjects(base_path: str = WESAD_PATH) -> List[str]:\n",
        "    \"\"\"\n",
        "    List all subject folders like 'S2', 'S3', ...\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(base_path):\n",
        "        raise FileNotFoundError(f\"WESAD base path not found: {base_path}\")\n",
        "    return sorted(\n",
        "        d for d in os.listdir(base_path)\n",
        "        if d.startswith(\"S\") and os.path.isdir(os.path.join(base_path, d))\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymUrzV-0lSZ5",
        "outputId": "5d8ea814-b881-4dc8-c4d8-7e72ee54f866"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wesad_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Importing loader & list subjects\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YcB4adTOslNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import wesad_loader\n",
        "importlib.reload(wesad_loader)\n",
        "\n",
        "from wesad_loader import load_subject_e4, list_subjects\n",
        "\n",
        "print(os.listdir())  # sanity\n",
        "print(list_subjects(\"data/WESAD\"))\n",
        "\n",
        "s2 = load_subject_e4(\"S2\", base_path=\"data/WESAD\")\n",
        "for k, v in s2.items():\n",
        "    if k == \"meta\":\n",
        "        print(k, v)\n",
        "    else:\n",
        "        print(k, type(v), getattr(v, \"shape\", None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_3sGZcrsnKn",
        "outputId": "ddfadcea-b160-43bf-fe29-7c8d12a171c2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cp322_FINAL.ipynb', 'wesad_loader.py', '.ipynb_checkpoints', '.git', 'data', 'README.md', 'DATASET', '__pycache__', 'CP322-Final-Project-Group-9']\n",
            "['S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9']\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/ACC.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/BVP.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/EDA.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/HR.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/IBI.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/TEMP.csv\n",
            "[wesad_loader] reading data/WESAD/S2/S2_E4_Data/tags.csv\n",
            "[wesad_loader] WARNING: data/WESAD/S2/S2_E4_Data/tags.csv is 0 bytes (empty)\n",
            "acc <class 'numpy.ndarray'> (251972, 3)\n",
            "bvp <class 'numpy.ndarray'> (503945,)\n",
            "eda <class 'numpy.ndarray'> (31496,)\n",
            "hr <class 'numpy.ndarray'> (7867,)\n",
            "ibi <class 'numpy.ndarray'> (3459, 2)\n",
            "temp <class 'numpy.ndarray'> (31498,)\n",
            "tags <class 'numpy.ndarray'> (0,)\n",
            "meta {'.csv files in this archive are in the following format': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yv3yApQPPk5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was way too large to add to the github, so a way to work around this for reproducibility purposes is to essentially just use your own Kaggle API JSON so u can pull from their database\n",
        "\n",
        "Will go into more detail later, but for now\n",
        "\n",
        "1. Go to Kaggle settings\n",
        "2. Create API token\n",
        "3. Manually bind their own kaggle.JSON if needed\n",
        "4. Upload it the same way to Colab when promted in the cell above.\n"
      ],
      "metadata": {
        "id": "Jy8Ibo3VPeQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2ndary Dataset I used:"
      ],
      "metadata": {
        "id": "uh8SezkK0IGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondary dataset WESAD is 3GB+ and cannot be uploaded to colab or pushed to the git due to such a large file size, so in order to keep this reproducible, I am going to keep this here so that the team will be able to retrieve the data at runtime from the kaggle servers."
      ],
      "metadata": {
        "id": "jp9ou8zGDGAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Affective Economics Dataset\n",
        "---\n"
      ],
      "metadata": {
        "id": "W-4ErWzj_EYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Repo contents:\", os.listdir())\n",
        "print(\"DATASET contents:\", os.listdir(\"DATASET\"))\n",
        "\n",
        "df = pd.read_csv(\"DATASET/AE_investment_dataset.csv\")\n",
        "df.head()\n",
        "df.info()\n",
        "df.isna().mean().sort_values().head(20)\n",
        "df.columns.tolist()\n",
        "for c in df.columns:\n",
        "    print(c)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHgjKOa5tQe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6d038aa5-63dc-4c1b-818f-2499cdc883ca"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/CP322-Final-Project-Group-9\n",
            "Repo contents: ['cp322_FINAL.ipynb', 'wesad_loader.py', '.ipynb_checkpoints', '.git', 'data', 'README.md', 'DATASET', '__pycache__', 'CP322-Final-Project-Group-9']\n",
            "DATASET contents: ['AE_investment_dataset.csv']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30 entries, 0 to 29\n",
            "Columns: 364 entries, Participant_code to SCR_AnticipatoryS4_T10\n",
            "dtypes: float64(356), int64(5), object(3)\n",
            "memory usage: 85.4+ KB\n",
            "Participant_code\n",
            "Age\n",
            "Gender\n",
            "Nationality\n",
            "Ethnicity\n",
            "Played_stock_market\n",
            "Played_in_years\n",
            "Played_how_often\n",
            "Stock_amount_S1_T1\n",
            "Stock_amount_S1_T2\n",
            "Stock_amount_S1_T3\n",
            "Stock_amount_S1_T4\n",
            "Stock_amount_S1_T5\n",
            "Stock_amount_S1_T6\n",
            "Stock_amount_S1_T7\n",
            "Stock_amount_S1_T8\n",
            "Stock_amount_S1_T9\n",
            "Stock_amount_S1_T10\n",
            "Stock_amount_S2_T1\n",
            "Stock_amount_S2_T2\n",
            "Stock_amount_S2_T3\n",
            "Stock_amount_S2_T4\n",
            "Stock_amount_S2_T5\n",
            "Stock_amount_S2_T6\n",
            "Stock_amount_S2_T7\n",
            "Stock_amount_S2_T8\n",
            "Stock_amount_S2_T9\n",
            "Stock_amount_S2_T10\n",
            "Stock_amount_S3_T1\n",
            "Stock_amount_S3_T2\n",
            "Stock_amount_S3_T3\n",
            "Stock_amount_S3_T4\n",
            "Stock_amount_S3_T5\n",
            "Stock_amount_S3_T6\n",
            "Stock_amount_S3_T7\n",
            "Stock_amount_S3_T8\n",
            "Stock_amount_S3_T9\n",
            "Stock_amount_S3_T10\n",
            "Stock_amount_S4_T1\n",
            "Stock_amount_S4_T2\n",
            "Stock_amount_S4_T3\n",
            "Stock_amount_S4_T4\n",
            "Stock_amount_S4_T5\n",
            "Stock_amount_S4_T6\n",
            "Stock_amount_S4_T7\n",
            "Stock_amount_S4_T8\n",
            "Stock_amount_S4_T9\n",
            "Stock_amount_S4_T10\n",
            "Total_money_S1_T1\n",
            "Total_money_S1_T2\n",
            "Total_money_S1_T3\n",
            "Total_money_S1_T4\n",
            "Total_money_S1_T5\n",
            "Total_money_S1_T6\n",
            "Total_money_S1_T7\n",
            "Total_money_S1_T8\n",
            "Total_money_S1_T9\n",
            "Total_money_S1_T10\n",
            "Total_money_S2_T1\n",
            "Total_money_S2_T2\n",
            "Total_money_S2_T3\n",
            "Total_money_S2_T4\n",
            "Total_money_S2_T5\n",
            "Total_money_S2_T6\n",
            "Total_money_S2_T7\n",
            "Total_money_S2_T8\n",
            "Total_money_S2_T9\n",
            "Total_money_S2_T10\n",
            "Total_money_S3_T1\n",
            "Total_money_S3_T2\n",
            "Total_money_S3_T3\n",
            "Total_money_S3_T4\n",
            "Total_money_S3_T5\n",
            "Total_money_S3_T6\n",
            "Total_money_S3_T7\n",
            "Total_money_S3_T8\n",
            "Total_money_S3_T9\n",
            "Total_money_S3_T10\n",
            "Total_money_S4_T1\n",
            "Total_money_S4_T2\n",
            "Total_money_S4_T3\n",
            "Total_money_S4_T4\n",
            "Total_money_S4_T5\n",
            "Total_money_S4_T6\n",
            "Total_money_S4_T7\n",
            "Total_money_S4_T8\n",
            "Total_money_S4_T9\n",
            "Total_money_S4_T10\n",
            "Money_in_stocks_S1_T1\n",
            "Money_in_stocks_S1_T2\n",
            "Money_in_stocks_S1_T3\n",
            "Money_in_stocks_S1_T4\n",
            "Money_in_stocks_S1_T5\n",
            "Money_in_stocks_S1_T6\n",
            "Money_in_stocks_S1_T7\n",
            "Money_in_stocks_S1_T8\n",
            "Money_in_stocks_S1_T9\n",
            "Money_in_stocks_S1_T10\n",
            "Money_in_stocks_S2_T1\n",
            "Money_in_stocks_S2_T2\n",
            "Money_in_stocks_S2_T3\n",
            "Money_in_stocks_S2_T4\n",
            "Money_in_stocks_S2_T5\n",
            "Money_in_stocks_S2_T6\n",
            "Money_in_stocks_S2_T7\n",
            "Money_in_stocks_S2_T8\n",
            "Money_in_stocks_S2_T9\n",
            "Money_in_stocks_S2_T10\n",
            "Money_in_stocks_S3_T1\n",
            "Money_in_stocks_S3_T2\n",
            "Money_in_stocks_S3_T3\n",
            "Money_in_stocks_S3_T4\n",
            "Money_in_stocks_S3_T5\n",
            "Money_in_stocks_S3_T6\n",
            "Money_in_stocks_S3_T7\n",
            "Money_in_stocks_S3_T8\n",
            "Money_in_stocks_S3_T9\n",
            "Money_in_stocks_S3_T10\n",
            "Money_in_stocks_S4_T1\n",
            "Money_in_stocks_S4_T2\n",
            "Money_in_stocks_S4_T3\n",
            "Money_in_stocks_S4_T4\n",
            "Money_in_stocks_S4_T5\n",
            "Money_in_stocks_S4_T6\n",
            "Money_in_stocks_S4_T7\n",
            "Money_in_stocks_S4_T8\n",
            "Money_in_stocks_S4_T9\n",
            "Money_in_stocks_S4_T10\n",
            "PANAS_Initial_interested\n",
            "PANAS_Initial_distressed\n",
            "PANAS_Initial_excited\n",
            "PANAS_Initial_upset\n",
            "PANAS_Initial_strong\n",
            "PANAS_Initial_guilty\n",
            "PANAS_Initial_scared\n",
            "PANAS_Initial_hostile\n",
            "PANAS_Initial_enthusiastic\n",
            "PANAS_Initial_proud\n",
            "PANAS_Initial_irritable\n",
            "PANAS_Initial_alert\n",
            "PANAS_Initial_ashamed\n",
            "PANAS_Initial_inspired\n",
            "PANAS_Initial_nervous\n",
            "PANAS_Initial_determined\n",
            "PANAS_Initial_attentive\n",
            "PANAS_Initial_jittery\n",
            "PANAS_Initial_active\n",
            "PANAS_Initial_afraid\n",
            "PANAS_Initial_positive\n",
            "PANAS_Initial_negative\n",
            "PANAS_S1_interested\n",
            "PANAS_S1_distressed\n",
            "PANAS_S1_excited\n",
            "PANAS_S1_upset\n",
            "PANAS_S1_strong\n",
            "PANAS_S1_guilty\n",
            "PANAS_S1_scared\n",
            "PANAS_S1_hostile\n",
            "PANAS_S1_enthusiastic\n",
            "PANAS_S1_proud\n",
            "PANAS_S1_irritable\n",
            "PANAS_S1_alert\n",
            "PANAS_S1_ashamed\n",
            "PANAS_S1_inspired\n",
            "PANAS_S1_nervous\n",
            "PANAS_S1_determined\n",
            "PANAS_S1_attentive\n",
            "PANAS_S1_jittery\n",
            "PANAS_S1_active\n",
            "PANAS_S1_afraid\n",
            "PANAS_S1_positive\n",
            "PANAS_S1_negative\n",
            "PANAS_S2_interested\n",
            "PANAS_S2_distressed\n",
            "PANAS_S2_excited\n",
            "PANAS_S2_upset\n",
            "PANAS_S2_strong\n",
            "PANAS_S2_guilty\n",
            "PANAS_S2_scared\n",
            "PANAS_S2_hostile\n",
            "PANAS_S2_enthusiastic\n",
            "PANAS_S2_proud\n",
            "PANAS_S2_irritable\n",
            "PANAS_S2_alert\n",
            "PANAS_S2_ashamed\n",
            "PANAS_S2_inspired\n",
            "PANAS_S2_nervous\n",
            "PANAS_S2_determined\n",
            "PANAS_S2_attentive\n",
            "PANAS_S2_jittery\n",
            "PANAS_S2_active\n",
            "PANAS_S2_afraid\n",
            "PANAS_S2_positive\n",
            "PANAS_S2_negative\n",
            "PANAS_S3_interested\n",
            "PANAS_S3_distressed\n",
            "PANAS_S3_excited\n",
            "PANAS_S3_upset\n",
            "PANAS_S3_strong\n",
            "PANAS_S3_guilty\n",
            "PANAS_S3_scared\n",
            "PANAS_S3_hostile\n",
            "PANAS_S3_enthusiastic\n",
            "PANAS_S3_proud\n",
            "PANAS_S3_irritable\n",
            "PANAS_S3_alert\n",
            "PANAS_S3_ashamed\n",
            "PANAS_S3_inspired\n",
            "PANAS_S3_nervous\n",
            "PANAS_S3_determined\n",
            "PANAS_S3_attentive\n",
            "PANAS_S3_jittery\n",
            "PANAS_S3_active\n",
            "PANAS_S3_afraid\n",
            "PANAS_S3_positive\n",
            "PANAS_S3_negative\n",
            "PANAS_S4_interested\n",
            "PANAS_S4_distressed\n",
            "PANAS_S4_excited\n",
            "PANAS_S4_upset\n",
            "PANAS_S4_strong\n",
            "PANAS_S4_guilty\n",
            "PANAS_S4_scared\n",
            "PANAS_S4_hostile\n",
            "PANAS_S4_enthusiastic\n",
            "PANAS_S4_proud\n",
            "PANAS_S4_irritable\n",
            "PANAS_S4_alert\n",
            "PANAS_S4_ashamed\n",
            "PANAS_S4_inspired\n",
            "PANAS_S4_nervous\n",
            "PANAS_S4_determined\n",
            "PANAS_S4_attentive\n",
            "PANAS_S4_jittery\n",
            "PANAS_S4_active\n",
            "PANAS_S4_afraid\n",
            "PANAS_S4_positive\n",
            "PANAS_S4_negative\n",
            "stock_fluctuation_S1_T2\n",
            "stock_fluctuation_S1_T3\n",
            "stock_fluctuation_S1_T4\n",
            "stock_fluctuation_S1_T5\n",
            "stock_fluctuation_S1_T6\n",
            "stock_fluctuation_S1_T7\n",
            "stock_fluctuation_S1_T8\n",
            "stock_fluctuation_S1_T9\n",
            "stock_fluctuation_S1_T10\n",
            "stock_fluctuation_S2_T2\n",
            "stock_fluctuation_S2_T3\n",
            "stock_fluctuation_S2_T4\n",
            "stock_fluctuation_S2_T5\n",
            "stock_fluctuation_S2_T6\n",
            "stock_fluctuation_S2_T7\n",
            "stock_fluctuation_S2_T8\n",
            "stock_fluctuation_S2_T9\n",
            "stock_fluctuation_S2_T10\n",
            "stock_fluctuation_S3_T2\n",
            "stock_fluctuation_S3_T3\n",
            "stock_fluctuation_S3_T4\n",
            "stock_fluctuation_S3_T5\n",
            "stock_fluctuation_S3_T6\n",
            "stock_fluctuation_S3_T7\n",
            "stock_fluctuation_S3_T8\n",
            "stock_fluctuation_S3_T9\n",
            "stock_fluctuation_S3_T10\n",
            "stock_fluctuation_S4_T2\n",
            "stock_fluctuation_S4_T3\n",
            "stock_fluctuation_S4_T4\n",
            "stock_fluctuation_S4_T5\n",
            "stock_fluctuation_S4_T6\n",
            "stock_fluctuation_S4_T7\n",
            "stock_fluctuation_S4_T8\n",
            "stock_fluctuation_S4_T9\n",
            "stock_fluctuation_S4_T10\n",
            "Mean_Return_S1_T2\n",
            "Mean_Return_S1_T3\n",
            "Mean_Return_S1_T4\n",
            "Mean_Return_S1_T5\n",
            "Mean_Return_S1_T6\n",
            "Mean_Return_S1_T7\n",
            "Mean_Return_S1_T8\n",
            "Mean_Return_S1_T9\n",
            "Mean_Return_S1_T10\n",
            "Mean_Return_S2_T2\n",
            "Mean_Return_S2_T3\n",
            "Mean_Return_S2_T4\n",
            "Mean_Return_S2_T5\n",
            "Mean_Return_S2_T6\n",
            "Mean_Return_S2_T7\n",
            "Mean_Return_S2_T8\n",
            "Mean_Return_S2_T9\n",
            "Mean_Return_S2_T10\n",
            "Mean_Return_S3_T2\n",
            "Mean_Return_S3_T3\n",
            "Mean_Return_S3_T4\n",
            "Mean_Return_S3_T5\n",
            "Mean_Return_S3_T6\n",
            "Mean_Return_S3_T7\n",
            "Mean_Return_S3_T8\n",
            "Mean_Return_S3_T9\n",
            "Mean_Return_S3_T10\n",
            "Mean_Return_S4_T2\n",
            "Mean_Return_S4_T3\n",
            "Mean_Return_S4_T4\n",
            "Mean_Return_S4_T5\n",
            "Mean_Return_S4_T6\n",
            "Mean_Return_S4_T7\n",
            "Mean_Return_S4_T8\n",
            "Mean_Return_S4_T9\n",
            "Mean_Return_S4_T10\n",
            "Overall_mean_return_percentage\n",
            "Total_return_S1\n",
            "Total_return_S2\n",
            "Total_return_S3\n",
            "Total_return_S4\n",
            "Overall_Total_return\n",
            "Residual_PANAS_S1_positive\n",
            "Residual_PANAS_S2_positive\n",
            "Residual_PANAS_S3_positive\n",
            "Residual_PANAS_S4_positive\n",
            "Residual_PANAS_S1_negative\n",
            "Residual_PANAS_S2_negative\n",
            "Residual_PANAS_S3_negative\n",
            "Residual_PANAS_S4_negative\n",
            "SCR_AnticipatoryS1_T1\n",
            "SCR_AnticipatoryS1_T2\n",
            "SCR_AnticipatoryS1_T3\n",
            "SCR_AnticipatoryS1_T4\n",
            "SCR_AnticipatoryS1_T5\n",
            "SCR_AnticipatoryS1_T6\n",
            "SCR_AnticipatoryS1_T7\n",
            "SCR_AnticipatoryS1_T8\n",
            "SCR_AnticipatoryS1_T9\n",
            "SCR_AnticipatoryS1_T10\n",
            "SCR_AnticipatoryS2_T1\n",
            "SCR_AnticipatoryS2_T2\n",
            "SCR_AnticipatoryS2_T3\n",
            "SCR_AnticipatoryS2_T4\n",
            "SCR_AnticipatoryS2_T5\n",
            "SCR_AnticipatoryS2_T6\n",
            "SCR_AnticipatoryS2_T7\n",
            "SCR_AnticipatoryS2_T8\n",
            "SCR_AnticipatoryS2_T9\n",
            "SCR_AnticipatoryS2_T10\n",
            "SCR_AnticipatoryS3_T1\n",
            "SCR_AnticipatoryS3_T2\n",
            "SCR_AnticipatoryS3_T3\n",
            "SCR_AnticipatoryS3_T4\n",
            "SCR_AnticipatoryS3_T5\n",
            "SCR_AnticipatoryS3_T6\n",
            "SCR_AnticipatoryS3_T7\n",
            "SCR_AnticipatoryS3_T8\n",
            "SCR_AnticipatoryS3_T9\n",
            "SCR_AnticipatoryS3_T10\n",
            "SCR_AnticipatoryS4_T1\n",
            "SCR_AnticipatoryS4_T2\n",
            "SCR_AnticipatoryS4_T3\n",
            "SCR_AnticipatoryS4_T4\n",
            "SCR_AnticipatoryS4_T5\n",
            "SCR_AnticipatoryS4_T6\n",
            "SCR_AnticipatoryS4_T7\n",
            "SCR_AnticipatoryS4_T8\n",
            "SCR_AnticipatoryS4_T9\n",
            "SCR_AnticipatoryS4_T10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quick note bc I didn't know what PANAS meant:\n",
        "- PANAS refers to the Positive and Negative Affect Schedule, a widely used psychological scale that measures an individual's mood by assessing both positive and negative emotions. Developed in 1988 by Watson, Clark, and Tellegen, it's a 20-item self-report measure used in research and clinical settings to gauge how frequently someone experiences emotions like interest, joy, enthusiasm (positive affect) versus feelings of distress, sadness, and nervousness (negative affect).\n",
        "## How it works\n",
        "- 20 items: The scale consists of 20 words that describe different feelings and emotions.\n",
        "- Two dimensions: These items are separated into two subscales: one for positive affect (PA) and one for negative affect (NA).\n",
        "- Rating scale: Participants rate how they felt about each item over a specific time frame (e.g., \"right now,\" \"today,\" \"over the past few weeks\") on a 5-point scale.\n",
        "- Scoring: Each positive and negative item is scored individually. The total positive score and total negative score are then calculated. A higher positive score indicates more positive affect, while a higher negative score indicates more negative affect."
      ],
      "metadata": {
        "id": "KPuQaXQjy8wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a per-trial table with:\n",
        "1. inputs per step:\n",
        "  - money_in_stocks\n",
        "  - mean_return\n",
        "  - stock_fluctuation\n",
        "  - scr_anticipatory\n",
        "2. Static inputs:\n",
        "3. Target\n",
        "  - Whether they invested in the stock (money_in_stocks > 0 -> 1 else 0)"
      ],
      "metadata": {
        "id": "xJg0a9hQw5Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle Dataset for WESAD\n",
        "\n",
        "-- note: ref doc later"
      ],
      "metadata": {
        "id": "pU7JKcLUQ-Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal WESAD pipeline\n",
        "- loading dataset, segmenting data into windows, computing basics, labelling windows as baselines vs stress.\n",
        "- using dataframe and constructing just like we do later on for the Bath dataset"
      ],
      "metadata": {
        "id": "kGH50BNvm3q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Loading Affective Economics Dataset\n",
        "---"
      ],
      "metadata": {
        "id": "NCFO4zqp_aRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Repo contents:\", os.listdir())\n",
        "print(\"DATASET contents:\", os.listdir(\"DATASET\"))\n",
        "\n",
        "df = pd.read_csv(\"DATASET/AE_investment_dataset.csv\")\n",
        "df.info()\n",
        "\n",
        "# 0. ID & static columns\n",
        "id_cols = [\"Participant_code\", \"Age\", \"Gender\", \"Nationality\", \"Ethnicity\", \"Played_stock_market\"]\n",
        "\n",
        "# 1. Grab trial-level columns by prefix\n",
        "stock_cols = [c for c in df.columns if c.startswith(\"Money_in_stocks_S\")]\n",
        "scr_cols   = [c for c in df.columns if c.startswith(\"SCR_AnticipatoryS\")]\n",
        "ret_cols   = [c for c in df.columns if c.startswith(\"Mean_Return_S\")]\n",
        "fluc_cols  = [c for c in df.columns if c.startswith(\"stock_fluctuation_S\")]\n",
        "\n",
        "print(\"n_stock_cols:\", len(stock_cols))\n",
        "print(\"n_scr_cols:\", len(scr_cols))\n",
        "print(\"n_return_cols:\", len(ret_cols))\n",
        "print(\"n_fluctuation_cols:\", len(fluc_cols))\n",
        "\n",
        "# 2. Map (session, trial) -> column name\n",
        "def build_lookup(cols, prefix):\n",
        "    lookup = {}\n",
        "    for c in cols:\n",
        "        # e.g. Money_in_stocks_S1_T3  →  session=1, trial=3\n",
        "        m = re.match(rf\"{re.escape(prefix)}(\\d+)_T(\\d+)$\", c)\n",
        "        if m:\n",
        "            s = int(m.group(1))   # session number\n",
        "            t = int(m.group(2))   # trial number within session\n",
        "            lookup[(s, t)] = c\n",
        "    return lookup\n",
        "\n",
        "stock_map = build_lookup(stock_cols, \"Money_in_stocks_S\")\n",
        "scr_map   = build_lookup(scr_cols,   \"SCR_AnticipatoryS\")\n",
        "ret_map   = build_lookup(ret_cols,   \"Mean_Return_S\")\n",
        "fluc_map  = build_lookup(fluc_cols,  \"stock_fluctuation_S\")\n",
        "\n",
        "print(\"number of keys in stock_map:\", len(stock_map))\n",
        "print(\"some keys from stock_map:\", list(stock_map.items())[:5])\n"
      ],
      "metadata": {
        "id": "QSaZ40fmqL5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed6e061-6f40-412d-b9f0-78f9f81d3e8a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/CP322-Final-Project-Group-9\n",
            "Repo contents: ['cp322_FINAL.ipynb', 'wesad_loader.py', '.ipynb_checkpoints', '.git', 'data', 'README.md', 'DATASET', '__pycache__', 'CP322-Final-Project-Group-9']\n",
            "DATASET contents: ['AE_investment_dataset.csv']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30 entries, 0 to 29\n",
            "Columns: 364 entries, Participant_code to SCR_AnticipatoryS4_T10\n",
            "dtypes: float64(356), int64(5), object(3)\n",
            "memory usage: 85.4+ KB\n",
            "n_stock_cols: 40\n",
            "n_scr_cols: 40\n",
            "n_return_cols: 36\n",
            "n_fluctuation_cols: 36\n",
            "number of keys in stock_map: 40\n",
            "some keys from stock_map: [((1, 1), 'Money_in_stocks_S1_T1'), ((1, 2), 'Money_in_stocks_S1_T2'), ((1, 3), 'Money_in_stocks_S1_T3'), ((1, 4), 'Money_in_stocks_S1_T4'), ((1, 5), 'Money_in_stocks_S1_T5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Long format trial table"
      ],
      "metadata": {
        "id": "84nI2l_vqNg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aD-mtCv7_vS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # carry participant-level info\n",
        "    base = {col: row[col] for col in id_cols}\n",
        "\n",
        "    for (s, t) in sorted(stock_map.keys()):\n",
        "        rec = dict(base)\n",
        "        rec[\"session\"] = s\n",
        "        rec[\"trial_in_session\"] = t\n",
        "        rec[\"global_trial\"] = (s - 1) * 10 + t  # 1..40\n",
        "\n",
        "        rec[\"money_in_stocks\"] = row[stock_map[(s, t)]]\n",
        "        rec[\"scr_anticipatory\"] = row[scr_map[(s, t)]]\n",
        "\n",
        "        # Some (session, trial) combos might not have return/fluctuation\n",
        "        rec[\"mean_return\"] = row[ret_map[(s, t)]] if (s, t) in ret_map else pd.NA\n",
        "        rec[\"stock_fluctuation\"] = row[fluc_map[(s, t)]] if (s, t) in fluc_map else pd.NA\n",
        "\n",
        "        rows.append(rec)\n",
        "\n",
        "long_df = pd.DataFrame(rows)\n",
        "\n",
        "# Target: did they invest at all?\n",
        "long_df[\"invested\"] = (long_df[\"money_in_stocks\"] > 0).astype(int)\n",
        "\n",
        "print(long_df.shape)\n",
        "long_df.head()\n",
        "print(long_df[\"invested\"].value_counts())\n"
      ],
      "metadata": {
        "id": "FhdtP55eqN1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49b7b69-6220-42b0-ffea-c1168bc82e26"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 14)\n",
            "invested\n",
            "1    1079\n",
            "0     121\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Cleaning + Basic Features\n"
      ],
      "metadata": {
        "id": "qebdov_vqPgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Drop rows where our key features are missing\n",
        "key_features = [\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\", \"money_in_stocks\"]\n",
        "\n",
        "clean_df = long_df.dropna(subset=key_features).copy()\n",
        "\n",
        "# Convert types to numeric\n",
        "for col in key_features:\n",
        "    clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
        "\n",
        "# Drop again if any become NA\n",
        "clean_df = clean_df.dropna(subset=key_features)\n",
        "\n",
        "# Target\n",
        "y = clean_df[\"invested\"].astype(int)\n",
        "\n",
        "# Features: minimal baseline\n",
        "X = clean_df[[\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\"]]\n",
        "\n",
        "print(\"Clean shape:\", clean_df.shape)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "tOjbGQh-qPwK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "59c955ce-f936-45f9-b52f-84feb8700624"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean shape: (1080, 14)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   scr_anticipatory  mean_return  stock_fluctuation\n",
              "1             0.028        0.045                2.0\n",
              "2             0.232       -0.046                1.0\n",
              "3             0.954        0.370                2.0\n",
              "4             0.000        0.046                2.0\n",
              "5             0.858        0.001                1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1dd3c78d-e606-4c11-a791-24aa4d02caf6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scr_anticipatory</th>\n",
              "      <th>mean_return</th>\n",
              "      <th>stock_fluctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.028</td>\n",
              "      <td>0.045</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.232</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.954</td>\n",
              "      <td>0.370</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.046</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.858</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dd3c78d-e606-4c11-a791-24aa4d02caf6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1dd3c78d-e606-4c11-a791-24aa4d02caf6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1dd3c78d-e606-4c11-a791-24aa4d02caf6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9b5af486-a570-454e-87fb-8f049c12bd97\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b5af486-a570-454e-87fb-8f049c12bd97')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9b5af486-a570-454e-87fb-8f049c12bd97 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 1080,\n  \"fields\": [\n    {\n      \"column\": \"scr_anticipatory\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18730514125534767,\n        \"min\": 0.0,\n        \"max\": 1.288,\n        \"num_unique_values\": 262,\n        \"samples\": [\n          0.226,\n          0.16,\n          0.289\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_return\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17854312209951967,\n        \"min\": -0.675,\n        \"max\": 0.882,\n        \"num_unique_values\": 482,\n        \"samples\": [\n          -0.024,\n          0.508,\n          0.339\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stock_fluctuation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4971342030248064,\n        \"min\": 1.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Baseline model\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "L4zrnPS4wpo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bqyfyroHwr5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Scale features again (safe to reuse)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "k_bUH3aZwsOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Cell\n"
      ],
      "metadata": {
        "id": "_nzl-aYc37DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#scale data again\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "JZDBGFsQ3843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Work on a copy, sorted by participant + time\n",
        "scr_df = clean_df.sort_values([\"Participant_code\", \"session\", \"trial_in_session\"]).copy()\n",
        "\n",
        "grp = scr_df.groupby(\"Participant_code\")\n",
        "\n",
        "# Participant-level mean/std and z-score\n",
        "scr_df[\"scr_mean_p\"] = grp[\"scr_anticipatory\"].transform(\"mean\")\n",
        "scr_df[\"scr_std_p\"]  = grp[\"scr_anticipatory\"].transform(\"std\")\n",
        "scr_df[\"scr_z\"] = (scr_df[\"scr_anticipatory\"] - scr_df[\"scr_mean_p\"]) / scr_df[\"scr_std_p\"]\n",
        "\n",
        "# Lags within each participant\n",
        "scr_df[\"scr_lag1\"] = grp[\"scr_anticipatory\"].shift(1)\n",
        "scr_df[\"scr_lag2\"] = grp[\"scr_anticipatory\"].shift(2)\n",
        "\n",
        "# Changes vs previous trials\n",
        "scr_df[\"scr_delta1\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag1\"]\n",
        "scr_df[\"scr_delta2\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag2\"]\n",
        "\n",
        "# Short-term rolling window stats (window=3 trials)\n",
        "scr_df[\"scr_roll_mean3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        ")\n",
        "scr_df[\"scr_roll_std3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "\n",
        "# Replace NaNs from lags / std=0 with 0 for now\n",
        "scr_feature_cols = [\n",
        "    \"scr_anticipatory\",\n",
        "    \"scr_z\",\n",
        "    \"scr_lag1\", \"scr_lag2\",\n",
        "    \"scr_delta1\", \"scr_delta2\",\n",
        "    \"scr_roll_mean3\", \"scr_roll_std3\",\n",
        "]\n",
        "\n",
        "scr_df[scr_feature_cols] = scr_df[scr_feature_cols].fillna(0.0)\n",
        "\n",
        "print(\"SCR feature shape:\", scr_df[scr_feature_cols].shape)\n",
        "scr_df[scr_feature_cols + [\"invested\"]].head()\n"
      ],
      "metadata": {
        "id": "0RYaY--sCQKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJFSLZtACP8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ==== Prepare data ====\n",
        "X_scr = scr_df[scr_feature_cols].values.astype(\"float32\")\n",
        "y_scr = scr_df[\"invested\"].values.astype(\"int64\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scr, y_scr, test_size=0.2, random_state=42, stratify=y_scr\n",
        ")\n",
        "\n",
        "# Standardize SCR features\n",
        "scaler_scr = StandardScaler()\n",
        "X_train_scaled = scaler_scr.fit_transform(X_train)\n",
        "X_val_scaled   = scaler_scr.transform(X_val)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor   = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_ds   = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "emb_dim   = 16\n",
        "\n",
        "# ==== Physiology encoder ====\n",
        "class PhysioEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=32, emb_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Full model: encoder + classifier head\n",
        "class PhysioModel(nn.Module):\n",
        "    def __init__(self, encoder, emb_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Linear(emb_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)        # [batch, emb_dim]\n",
        "        logits = self.classifier(z)  # [batch, 2]\n",
        "        return logits\n",
        "\n",
        "encoder = PhysioEncoder(input_dim, hidden_dim=32, emb_dim=emb_dim)\n",
        "model = PhysioModel(encoder, emb_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ==== Training loop ====\n",
        "n_epochs = 30\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_true  = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_true.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_true  = torch.cat(all_true).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_true, all_preds)\n",
        "    f1  = f1_score(all_true, all_preds)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "              f\"train_loss={np.mean(train_losses):.4f} | \"\n",
        "              f\"val_acc={acc:.3f} | val_f1={f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "dwpP5lQ3CTEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}