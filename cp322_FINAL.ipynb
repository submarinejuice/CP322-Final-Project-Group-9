{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/submarinejuice/CP322-Final-Project-Group-9/blob/Michelle-Main/cp322_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading Data CSV from repo so we always have it and dont have to manually import\n",
        "Michelle's addition"
      ],
      "metadata": {
        "id": "jKQJZUx8smAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/submarinejuice/CP322-Final-Project-Group-9\"\n",
        "REPO_NAME = \"CP322-Final-Project-Group-9\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    # First time in this Colab session: clone the repo\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    # Repo already there in this runtime: pull latest changes\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull\n",
        "    %cd /content\n",
        "\n",
        "# Move into repo so relative paths work\n",
        "%cd /content/{REPO_NAME}\n"
      ],
      "metadata": {
        "id": "a8X3zmLAslA5",
        "outputId": "dfc45ece-1ff4-450c-d2d8-2fe2a474065a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CP322-Final-Project-Group-9/CP322-Final-Project-Group-9\n",
            "Already up to date.\n",
            "/content\n",
            "/content/CP322-Final-Project-Group-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "verification\n"
      ],
      "metadata": {
        "id": "RoZfA2Q0JlB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())          # should show wesad_loader.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrMP16bSJl4f",
        "outputId": "9a848745-5e70-4fc5-a2df-da43b49d3d43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cp322_FINAL.ipynb', '.ipynb_checkpoints', '.git', 'data', 'README.md', 'DATASET', 'CP322-Final-Project-Group-9', 'kaggle.JSON']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Ensure folders exist\n",
        "os.makedirs('/content/.kaggle', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Upload your kaggle.json from laptop\n",
        "uploaded = files.upload()   # SELECT your kaggle.json (or kaggle.JSON etc.)\n",
        "fname = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", fname)\n",
        "\n",
        "# Move/rename it using Python (handles spaces & brackets)\n",
        "src = os.path.join('/content', fname)\n",
        "dst = '/content/.kaggle/kaggle.json'\n",
        "shutil.move(src, dst)\n",
        "\n",
        "# Fix permissions\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "# Sanity check: show contents (should have username + key)\n",
        "!cat /content/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "DQNE9sXFJfAu",
        "outputId": "bf7c2f39-346c-42fa-cb09-c8755bdab513"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3dba3f96-6d0e-4d89-8d12-98748388c4ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3dba3f96-6d0e-4d89-8d12-98748388c4ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.JSON to kaggle (3).JSON\n",
            "Uploaded: kaggle (3).JSON\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/kaggle (3).JSON'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kaggle (3).JSON' -> '/content/.kaggle/kaggle.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2626404871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/.kaggle/kaggle.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Fix permissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kaggle (3).JSON'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y2Ftu6I8f2Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yv3yApQPPk5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was way too large to add to the github, so a way to work around this for reproducibility purposes is to essentially just use your own Kaggle API JSON so u can pull from their database\n",
        "\n",
        "Will go into more detail later, but for now\n",
        "\n",
        "1. Go to Kaggle settings\n",
        "2. Create API token\n",
        "3. Manually bind their own kaggle.JSON if needed\n",
        "4. Upload it the same way to Colab when promted in the cell above.\n"
      ],
      "metadata": {
        "id": "Jy8Ibo3VPeQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/.kaggle', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # SELECT kaggle.json from your laptop\n",
        "\n",
        "# Get the actual uploaded filename\n",
        "fname = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", fname)\n",
        "\n",
        "# Move and rename it to the expected path\n",
        "!mv {fname} /content/.kaggle/kaggle.json\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "# show the file contents (should have username + key)\n",
        "!cat /content/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "pC684sJPP7uW",
        "outputId": "98a9c526-54fa-4a98-c720-3bd99688bac3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2a9fa3e-f227-4ef2-b3ae-eb1641c6d51c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2a9fa3e-f227-4ef2-b3ae-eb1641c6d51c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.JSON to kaggle (2).JSON\n",
            "Uploaded: kaggle (2).JSON\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `mv kaggle (2).JSON /content/.kaggle/kaggle.json'\n",
            "chmod: cannot access '/content/.kaggle/kaggle.json': No such file or directory\n",
            "cat: /content/.kaggle/kaggle.json: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2ndary Dataset I used:"
      ],
      "metadata": {
        "id": "uh8SezkK0IGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondary dataset WESAD is 3GB+ and cannot be uploaded to colab or pushed to the git due to such a large file size, so in order to keep this reproducible, I am going to keep this here so that the team will be able to retrieve the data at runtime from the kaggle servers."
      ],
      "metadata": {
        "id": "jp9ou8zGDGAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wesad_loader import load_subject_e4\n",
        "\n",
        "s2 = load_subject_e4(\"S2\", base_path=\"data/WESAD\")\n",
        "for k, v in s2.items():\n",
        "    print(k, type(v), len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "F2esz4MUJtag",
        "outputId": "b823a504-8355-40d9-8113-1c66bd708826"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wesad_loader'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2554062709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwesad_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_subject_e4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_subject_e4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/WESAD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wesad_loader'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8zHnE5G9tQLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Repo contents:\", os.listdir())\n",
        "print(\"DATASET contents:\", os.listdir(\"DATASET\"))\n",
        "\n",
        "df = pd.read_csv(\"DATASET/AE_investment_dataset.csv\")\n",
        "df.head()\n",
        "df.info()\n",
        "df.isna().mean().sort_values().head(20)\n",
        "df.columns.tolist()\n",
        "for c in df.columns:\n",
        "    print(c)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHgjKOa5tQe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quick note bc I didn't know what PANAS meant:\n",
        "- PANAS refers to the Positive and Negative Affect Schedule, a widely used psychological scale that measures an individual's mood by assessing both positive and negative emotions. Developed in 1988 by Watson, Clark, and Tellegen, it's a 20-item self-report measure used in research and clinical settings to gauge how frequently someone experiences emotions like interest, joy, enthusiasm (positive affect) versus feelings of distress, sadness, and nervousness (negative affect).\n",
        "## How it works\n",
        "- 20 items: The scale consists of 20 words that describe different feelings and emotions.\n",
        "- Two dimensions: These items are separated into two subscales: one for positive affect (PA) and one for negative affect (NA).\n",
        "- Rating scale: Participants rate how they felt about each item over a specific time frame (e.g., \"right now,\" \"today,\" \"over the past few weeks\") on a 5-point scale.\n",
        "- Scoring: Each positive and negative item is scored individually. The total positive score and total negative score are then calculated. A higher positive score indicates more positive affect, while a higher negative score indicates more negative affect."
      ],
      "metadata": {
        "id": "KPuQaXQjy8wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a per-trial table with:\n",
        "1. inputs per step:\n",
        "  - money_in_stocks\n",
        "  - mean_return\n",
        "  - stock_fluctuation\n",
        "  - scr_anticipatory\n",
        "2. Static inputs:\n",
        "3. Target\n",
        "  - Whether they invested in the stock (money_in_stocks > 0 -> 1 else 0)"
      ],
      "metadata": {
        "id": "xJg0a9hQw5Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle Dataset for WESAD\n",
        "\n",
        "-- note: ref doc later"
      ],
      "metadata": {
        "id": "pU7JKcLUQ-Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvf5RwFq_1UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KqMvMSQN_0iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal WESAD pipeline\n",
        "- loading dataset, segmenting data into windows, computing basics, labelling windows as baselines vs stress.\n",
        "- using dataframe and constructing just like we do later on for the Bath dataset"
      ],
      "metadata": {
        "id": "kGH50BNvm3q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 1. ID & static columns we carry along\n",
        "id_cols = [\"Participant_code\", \"Age\", \"Gender\", \"Nationality\", \"Ethnicity\", \"Played_stock_market\"]\n",
        "\n",
        "# 2. Find all trial-level columns by prefix\n",
        "stock_cols = [c for c in df.columns if c.startswith(\"Money_in_stocks_S\")]\n",
        "scr_cols   = [c for c in df.columns if c.startswith(\"SCR_AnticipatoryS\")]\n",
        "ret_cols   = [c for c in df.columns if c.startswith(\"Mean_Return_S\")]\n",
        "fluc_cols  = [c for c in df.columns if c.startswith(\"stock_fluctuation_S\")]\n",
        "\n",
        "print(\"n_stock_cols:\", len(stock_cols))\n",
        "print(\"n_scr_cols:\", len(scr_cols))\n",
        "print(\"n_return_cols:\", len(ret_cols))\n",
        "print(\"n_fluctuation_cols:\", len(fluc_cols))\n"
      ],
      "metadata": {
        "id": "2HIru1DWw3fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7t4uy8FAqLdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "54IFaSTk_zFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 0. ID & static columns\n",
        "id_cols = [\"Participant_code\", \"Age\", \"Gender\", \"Nationality\", \"Ethnicity\", \"Played_stock_market\"]\n",
        "\n",
        "# 1. Grab trial-level columns by prefix\n",
        "stock_cols = [c for c in df.columns if c.startswith(\"Money_in_stocks_S\")]\n",
        "scr_cols   = [c for c in df.columns if c.startswith(\"SCR_AnticipatoryS\")]\n",
        "ret_cols   = [c for c in df.columns if c.startswith(\"Mean_Return_S\")]\n",
        "fluc_cols  = [c for c in df.columns if c.startswith(\"stock_fluctuation_S\")]\n",
        "\n",
        "print(\"n_stock_cols:\", len(stock_cols))\n",
        "print(\"n_scr_cols:\", len(scr_cols))\n",
        "print(\"n_return_cols:\", len(ret_cols))\n",
        "print(\"n_fluctuation_cols:\", len(fluc_cols))\n",
        "\n",
        "# 2. Map (session, trial) -> column name\n",
        "def build_lookup(cols, prefix):\n",
        "    lookup = {}\n",
        "    for c in cols:\n",
        "        # e.g. Money_in_stocks_S1_T3  â†’  session=1, trial=3\n",
        "        m = re.match(rf\"{re.escape(prefix)}(\\d+)_T(\\d+)$\", c)\n",
        "        if m:\n",
        "            s = int(m.group(1))   # session number\n",
        "            t = int(m.group(2))   # trial number within session\n",
        "            lookup[(s, t)] = c\n",
        "    return lookup\n",
        "\n",
        "stock_map = build_lookup(stock_cols, \"Money_in_stocks_S\")\n",
        "scr_map   = build_lookup(scr_cols,   \"SCR_AnticipatoryS\")\n",
        "ret_map   = build_lookup(ret_cols,   \"Mean_Return_S\")\n",
        "fluc_map  = build_lookup(fluc_cols,  \"stock_fluctuation_S\")\n",
        "\n",
        "print(\"number of keys in stock_map:\", len(stock_map))\n",
        "print(\"some keys from stock_map:\", list(stock_map.items())[:5])\n"
      ],
      "metadata": {
        "id": "QSaZ40fmqL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "84nI2l_vqNg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # carry participant-level info\n",
        "    base = {col: row[col] for col in id_cols}\n",
        "\n",
        "    for (s, t) in sorted(stock_map.keys()):\n",
        "        rec = dict(base)\n",
        "        rec[\"session\"] = s\n",
        "        rec[\"trial_in_session\"] = t\n",
        "        rec[\"global_trial\"] = (s - 1) * 10 + t  # 1..40\n",
        "\n",
        "        rec[\"money_in_stocks\"] = row[stock_map[(s, t)]]\n",
        "        rec[\"scr_anticipatory\"] = row[scr_map[(s, t)]]\n",
        "\n",
        "        # Some (session, trial) combos might not have return/fluctuation\n",
        "        rec[\"mean_return\"] = row[ret_map[(s, t)]] if (s, t) in ret_map else pd.NA\n",
        "        rec[\"stock_fluctuation\"] = row[fluc_map[(s, t)]] if (s, t) in fluc_map else pd.NA\n",
        "\n",
        "        rows.append(rec)\n",
        "\n",
        "long_df = pd.DataFrame(rows)\n",
        "\n",
        "# Target: did they invest at all?\n",
        "long_df[\"invested\"] = (long_df[\"money_in_stocks\"] > 0).astype(int)\n",
        "\n",
        "print(long_df.shape)\n",
        "long_df.head()\n",
        "print(long_df[\"invested\"].value_counts())\n"
      ],
      "metadata": {
        "id": "FhdtP55eqN1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQZ-jR2ksOEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qebdov_vqPgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Drop rows where our key features are missing\n",
        "key_features = [\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\", \"money_in_stocks\"]\n",
        "\n",
        "clean_df = long_df.dropna(subset=key_features).copy()\n",
        "\n",
        "# Convert types to numeric\n",
        "for col in key_features:\n",
        "    clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
        "\n",
        "# Drop again if any become NA\n",
        "clean_df = clean_df.dropna(subset=key_features)\n",
        "\n",
        "# Target\n",
        "y = clean_df[\"invested\"].astype(int)\n",
        "\n",
        "# Features: minimal baseline\n",
        "X = clean_df[[\"scr_anticipatory\", \"mean_return\", \"stock_fluctuation\"]]\n",
        "\n",
        "print(\"Clean shape:\", clean_df.shape)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "tOjbGQh-qPwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_XS6beqJwpWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Baseline model\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "L4zrnPS4wpo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bqyfyroHwr5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Scale features again (safe to reuse)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "k_bUH3aZwsOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Cell\n"
      ],
      "metadata": {
        "id": "_nzl-aYc37DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#scale data again\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# MLP model: small but strong\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"MLP F1:\", f1_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "JZDBGFsQ3843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Work on a copy, sorted by participant + time\n",
        "scr_df = clean_df.sort_values([\"Participant_code\", \"session\", \"trial_in_session\"]).copy()\n",
        "\n",
        "grp = scr_df.groupby(\"Participant_code\")\n",
        "\n",
        "# Participant-level mean/std and z-score\n",
        "scr_df[\"scr_mean_p\"] = grp[\"scr_anticipatory\"].transform(\"mean\")\n",
        "scr_df[\"scr_std_p\"]  = grp[\"scr_anticipatory\"].transform(\"std\")\n",
        "scr_df[\"scr_z\"] = (scr_df[\"scr_anticipatory\"] - scr_df[\"scr_mean_p\"]) / scr_df[\"scr_std_p\"]\n",
        "\n",
        "# Lags within each participant\n",
        "scr_df[\"scr_lag1\"] = grp[\"scr_anticipatory\"].shift(1)\n",
        "scr_df[\"scr_lag2\"] = grp[\"scr_anticipatory\"].shift(2)\n",
        "\n",
        "# Changes vs previous trials\n",
        "scr_df[\"scr_delta1\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag1\"]\n",
        "scr_df[\"scr_delta2\"] = scr_df[\"scr_anticipatory\"] - scr_df[\"scr_lag2\"]\n",
        "\n",
        "# Short-term rolling window stats (window=3 trials)\n",
        "scr_df[\"scr_roll_mean3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        ")\n",
        "scr_df[\"scr_roll_std3\"] = grp[\"scr_anticipatory\"].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "\n",
        "# Replace NaNs from lags / std=0 with 0 for now\n",
        "scr_feature_cols = [\n",
        "    \"scr_anticipatory\",\n",
        "    \"scr_z\",\n",
        "    \"scr_lag1\", \"scr_lag2\",\n",
        "    \"scr_delta1\", \"scr_delta2\",\n",
        "    \"scr_roll_mean3\", \"scr_roll_std3\",\n",
        "]\n",
        "\n",
        "scr_df[scr_feature_cols] = scr_df[scr_feature_cols].fillna(0.0)\n",
        "\n",
        "print(\"SCR feature shape:\", scr_df[scr_feature_cols].shape)\n",
        "scr_df[scr_feature_cols + [\"invested\"]].head()\n"
      ],
      "metadata": {
        "id": "0RYaY--sCQKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJFSLZtACP8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ==== Prepare data ====\n",
        "X_scr = scr_df[scr_feature_cols].values.astype(\"float32\")\n",
        "y_scr = scr_df[\"invested\"].values.astype(\"int64\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scr, y_scr, test_size=0.2, random_state=42, stratify=y_scr\n",
        ")\n",
        "\n",
        "# Standardize SCR features\n",
        "scaler_scr = StandardScaler()\n",
        "X_train_scaled = scaler_scr.fit_transform(X_train)\n",
        "X_val_scaled   = scaler_scr.transform(X_val)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor   = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_ds   = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "emb_dim   = 16\n",
        "\n",
        "# ==== Physiology encoder ====\n",
        "class PhysioEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=32, emb_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Full model: encoder + classifier head\n",
        "class PhysioModel(nn.Module):\n",
        "    def __init__(self, encoder, emb_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Linear(emb_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)        # [batch, emb_dim]\n",
        "        logits = self.classifier(z)  # [batch, 2]\n",
        "        return logits\n",
        "\n",
        "encoder = PhysioEncoder(input_dim, hidden_dim=32, emb_dim=emb_dim)\n",
        "model = PhysioModel(encoder, emb_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ==== Training loop ====\n",
        "n_epochs = 30\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_true  = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_true.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_true  = torch.cat(all_true).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_true, all_preds)\n",
        "    f1  = f1_score(all_true, all_preds)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "              f\"train_loss={np.mean(train_losses):.4f} | \"\n",
        "              f\"val_acc={acc:.3f} | val_f1={f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "dwpP5lQ3CTEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}